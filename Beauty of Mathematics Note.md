# Beauty of Mathematics Notebook

Author: Haomin XU

------



##第1章：文字和语言 vs 数字和信息

* 通信等原理和信息传播的模型
  ```mermaid
  graph LR
  A[信息源] -->|编码|B[信道]
  B -->|解码|C[接受者]
  ```

* （信源）编码和最短编码

  在罗马体系的文字中，常用字短，生僻字长。在意型文字中，也是类似大都常用字笔画少，生僻字笔画多。

  ​

* 解码的规则，语法

  从字母到词的构词法（Morphology）是词的编码规则，语法则是语言的编码和解码规则。

  不过相比较而言，词可以被认为是有限而且封闭的集合，而语言则是无限和开放的集合。

  ==封闭集合可以有完备的编码解码规则。==

  ​

* 聚类

  古埃及的象形文字中，读音相同的词可能用同一种符号记录。

  ​

* 校验位

  犹太学者抄写《圣经》的时候，每一个希伯来字母对应一个数字，这样每行文字加起来便得到一个特殊的数字。这个数字便成了这一行的校验码。如果某行的校验码和原文中对不上，则说明这行中至少有一个抄写错误。

  ​

* 双语对照文本，语料库和机器翻译

  1. 信息的冗余是信息安全的保障。（罗赛塔石碑上的内容是同一信息用不同语言重复三次）

  2. 语言的数据（语料），尤其是双语或多语短对照语料对翻译至关重要，是机器翻译研究的基础。

     ​

* 多义性和利用上下文消除歧义性

  文字按照意思来类聚，最终会带来一些奇异性。利用上下文来消除部分歧义（Disambiguation）。

  ​


## 第2章：自然语言处理

* Alan Turing最早提出一种验证机器是否有智能的方法：让人和机器进行交流，如果人无法判断自己交流的对象是人还是机器，就说明这个机器有智能了。（图灵测试 Turing Test）

* 误区

  ​	20世纪50年代到70年代，计算机处理自然语言局限在人类学习语言的方式上，用电脑模拟人脑。「计算机理解自然语言」，分析语句和获取语义。

  局限性：

  1. 文法规则的数量大，来不及人工制定，甚至会出现矛盾。

    2.  自然语言和文法和计算机高级程序语言的文法不同。自然语言有词义和上下文			相关的特性（Context Dependent Grammar）。而程序语言是人为设计的，便于计算机解码的上文无关文法（Context Independent Grammar）。

        利用计算复杂度（Computational Complexity）来衡量算法的耗时。对于上下文无关文法，算法的复杂度基本上是语句长度的2次方；上下文有关文法则是6次方。

* 从规则到统计

  1970年后，统计语言学的出现使得自然语言处理重获新生。

  ​


## 第3章：统计语言模型

计算机处理自然语言，一个基本问题就是为自然语言这种上下文相关的特性建立数学模型（统计语言模型 Statistical Language Model）。



1. 用数学的方法描述语言规律

   一个句子是否合理，就看它的可能性大小如何。假定$S$表示某一个有意义的句子，有一连串特定顺序排列的词$w_1,w2,\ldots,w_n$组成，$n$为句子的长度。$S$在文本中出现的可能性为$P(S)$。
   $$
   P(S)=P(w_1,w2,\ldots,w_n)\\=P(w_1)\cdot P(w_2|w_1)\cdot P(w_3|w_1,w_2)\cdots P(w_n|w_1,w2,\ldots,w_{n-1})
   $$
   $P(w_n|w_1,w2,\ldots,w_{n-1})$的可能性太多无法，利用马尔可夫(Andrey Markov)假设进行估算。
   $$
   P(S)=P(w_1)\cdot P(w_2|w_1)\cdot P(w_3|w_2)\cdots P(w_n|w_{n-1}) \ \ \ \ \ \ \ (3.3) 
   $$
   在公式(3.3)对应的统计语言模型是二元模型（Bigram Model）。一个词由前面$N-1$个词决定，对应的模型为$N$元模型。

   由联合概率$P(w_{i-1},w_i)$和边缘概率$P(w_{i-1})$估计条件概率：
   $$
   P(w_i|w_{i-1})=\frac {P(w_{i-1},w_i)}{P(w_{i-1})}
   $$
   根据相对频度和大数定理，条件概率$P(w_i|w_{i-1})$可估计为：
   $$
   P(w_i|w_{i-1})\approx \frac{\#(w_{i-1},w_i)}{\#(w_{i-1})}
   $$
   $\#(w_{i-1},w_i)$理解为$w_{i-1},w_i$这对词统计在文本*（语料库 Corpus）*中前后相邻出现的次数。

   事实证明，统计语言模型比任何已知的借助某种规则的解决方法更有效。

   ​


2. 统计语言模型的工程诀窍

   * 高阶语言模型

     假定文本中的每个词$w_i$和前面$N-1$个词有关，而与更前面的词无关，则
     $$
     P(w_i|w_1,w2,\ldots,w_{i-1})=P(w_i|w_{i-n+1},w_{i-n+2},\cdots,w_{i-1})\ \ \ \ \ \ \ (3.10)
     $$
     这种假设称为$N-1$阶马尔可夫假设，对应的语言模型称为$N$元模型*(N-Gram Model)*。$N=1$的一元模型实际上是一个上下文无关的模型。而实际应用中最多的是$N=3$的三元模型。当模型从3到4乃至更高元，效果的提升不明显，而资源的耗费却增加得非常快。

   * 模型的训练、零概率问题和平滑方法

     使用语言模型需要知道模型中所有的条件概率（模型的参数）。通过对语料的统计，得到这些参数的过程称作模型的训练。

     如何正确地训练一个语言模型？

     一是增加数据量，因为有大数定理（*Law of Large Numbers*），所以足够多的观测值可以估计模型的参数。

     二是即使有足够多数据量仍然会出现的零概率或统计量不足的问题。训练统计语言模型就在于解决好统计样本不足时的概率估计问题。==古德-图灵估计==*（Good-Turing Estimate）*给出了这种情况下的估算概率的方法：对于未见的事件，不能认为发生的概率为零，因此从概率的总量*（Probability Mass）*中分配一个很小的比例给未见的事件。 需要将所有可见事件概率根据“越是不可信的统计折扣越多”的方式调小。

     假定预料库中出现$r$次的词有$N_r$个，未出现的词量数为$N_0$，语料库大小为$N$。
     $$
     N=\sum_{r=1}^{\infty} rN_r
     $$
     出现$r$次的词在整个语料库中的相对频度*（Relative Frequency）*是$rN_r/N$。以相对平度作为这些词的概率估计。

     根据古德-图灵估计，当$r$比较小时，需要用更小的概率$d_r$估计出现$r$次的词的概率。
     $$
     d_r=(r+1)\cdot N_{r+1}/N_r<r\cdot N_r/N_r\\ \sum_{r}d_r\cdot N_r=N
     $$
     根据Zipf's Law，$r$越大，词的数量$N_r$越小，即$N_{r+1}<N_r$。在实际的自然语言处理中，一般对出现次数超过某个阈值的词，频率不下调，只对出现次数低于阈值的词下调频率。下调得到的频率总和给未出现的词。

     Example：对于二元组$(w_{i-1},w_i)$的条件概率估计$P(w_i|w_{i-1})$做古德-图灵估计
     $$
     P(w_i|w_{i-1})=\begin{cases}f(w_i|w_{i-1}) \ \ \ \ \ \ \ \  \ \ \ if \#(w_{i-1},w_i)\geq T
     \\f_gt(w_i|w_{i-1}) \ \ \ \ \ \ \ \ if 0<\#(w_{i-1},w_i)<T
     \\Q(w_{i-1})\cdot f(w_i) \ \ \ otherwise
     \end{cases}
     $$
     其中$T$是一个阈值，一般在8-10左右，函数$f_{gt}()$表示经过古德-图灵估计后的相对频度。

     $$Q(w_i-1)=\frac {1-\sum_{w_i seen} P(w_i|w_{i-1})}{\sum_{w_i unseen}f(w_i)}$$

   * 语料的选取问题

     * 训练语料和模型应用的领域需相关

     * 训练数据越多越好

     * 对训练数据进行预处理

       ​


## 第4章：谈谈分词

词是表达语义的最小单位。对于西方拼音语言，词之间有明确的分界符(Delimit)。而一些亚洲语言（如中文），词之间没有明确的分界符。需要先对句子进行分词，才能做进一步的自然语言处理。

利用统计语言模型分词的方法，假定一个句子有几种分词方法，那么最好的一种分词方法应该保证分词完成后这个句子出现的概率最大。

实现技巧：动态规划（Dynamic Programming），维特比（Viterbi）算法。

* 分词的一致性

  简单依靠与人工分词的结果比较来衡量分词器等准确性很难且无意义。因为不同人对于分词的看法还有差异。

* 词的颗粒度和层次

  在机器翻译中，一般颗粒度大翻译效果好；在网页搜索中，小的颗粒度比大的颗粒度好。

  好的做法是，让一个分词器同时支持不同层次的词的分词：首先需要一个基本词表和一个复合词表。然后根据基本词表和复合词表各建立一个语言模型，如L1和L2。再分别根据词表和语言模型进行分词。（这样做保证的是即使在词表和语言模型两个数据库变化的情况下，分词器程序的相同）

* 分词器准确性

  分词的不一致性可以分为错误和颗粒度不一致。错误又分为越界型和覆盖型。

  近来中文分词工作的重点是做数据挖掘，不断完善复合词的词典。

  ​


## 第5章：隐含马尔可夫模型

1. 通信模型

```mermaid
graph LR
A[信息,上下文] -->|编码:S1,S2,S3...|B[信道]
B -->|解码:O1,O2,O3...|C[接受者]
```

$s_1,s_2,s_3,\cdots$表示信息源发出的信号。$o_1,o_2,o_3,\cdots$表示接收器接收到的信号。根据接收端的观测信号来推测信号源发生的信息，需要从所有的源信息中找到最可能产生出观测信号的那个信息。即：
$$
s_1,s_2,s_3,\cdots=Arg_{all \ s_1,s_2,s_3,\cdots}MaxP(s_1,s_2,s_3,\cdots|o_1,o_2,o_3,\cdots)
$$
$Arg$表示Argument，能获得最大值的那个信息串。利用贝叶斯公式，间接计算上面的概率：
$$
\frac {P(o_1,o_2,o_3,\cdots|s_1,s_2,s_3,\cdots)\cdot P(s_1,s_2,s_3,\cdots)}{P(o_1,o_2,o_3,\cdots)}
$$
一旦信息$o_1,o_2,o_3,\cdots$产生了就不会改变，因此$P(o_1,o_2,o_3,\cdots)$就是一个可以忽略的常数。上述公式可以等价为：
$$
P(o_1,o_2,o_3,\cdots|s_1,s_2,s_3,\cdots)\cdot P(s_1,s_2,s_3,\cdots)
$$
这个公式可以用隐含马尔可夫模型*（Hidden Markov Model）*来估计。

2. 隐含马尔可夫模型

   * 马尔可夫链

     从相对静态的随机变量到随机变量到时间序列$s_1,s_2,s_3,\cdots,s_t,\cdots$，即随机过程（动态）。首先，在任意一个时刻$t$，对应的状态$s_t$都是随机的。第二，任一状态$s_t$的取值都可能和周围其他的状态相关。马尔可夫提出一种简化假设，即随机过程中各个状态$s_t$的概率分布只与它的前一个状态$s_{t-1}$有关，即$P(s_t|s_1,s_2,s_3,\cdots,{s_t-1})=P(s_t|s_{t-1})$。符合这个假设的随机过程被称为马尔可夫过程，或马尔可夫链。

   * [隐含马尔可夫模型](https://www.zhihu.com/question/20962240)

     任意时刻$t​$的状态$s_t​$不可见。观察者无法通过观察一个状态序列$s_1,s_2,s_3,\cdots,s_t​$来推测转移概率等参数。但隐含马尔可夫模型在每个时刻$t​$会输出一个符号$，o_t​$且$o_t​$仅跟$s_t​$相关（独立输出假设）。基于马尔可夫假设和独立输出假设，某个特定状态序列$s_1,s_2,s_3,\cdots​$产生输出符号$o_1,o_2,o_3,\cdots​$的概率为：
     $$
     P(s_1,s_2,s_3,\cdots,o_1,o_2,o_3,\cdots)=\prod _t P(s_t|s_{t-1})\cdot P(o_t|s_t)
     $$
     将马尔可夫假设和独立输出假设应用：
     $$
     P(o_1,o_2,o_3,\cdots|s_1,s_2,s_3,\cdots)=\prod_t P(o_t|s_t) \\P(s_1,s_2,s_3,\cdots)=\prod_t P(s_t|s_{t-1})
     $$
     在公式中$P(s_1,s_2,s_3,\cdots)$是语言模型。转移概率*（Transition Probability）*：$P(s_t|s_{t-1})$。生成概率*（Generation Probability）*：$P(o_t|s_t)$。

   ​

   * 隐含马尔可夫的三个基本问题

     1. 给定一个模型，如何计算某个特定的输出序列的概率。

        Forward-Backward算法

     2. 给定一个模型和某个特定的输出序列，如何找到最可能产生这个输出的状态序列。

        维特比算法

     3. 给定足够量的观测数据，如何估计隐含马尔可夫模型的参数。

        模型训练，计算或估算转移概率和生成概率。

        方法1，有监督的训练方法*（Supervised Training）*，前提需要大量人工标记*（Human Annotated）*数据。

        方法2，无监督的训练方法，仅通过大量观测到的信号$o_1,o_2,o_3,\cdots$就能推算模型参数转移概率和生成概率。主要使用鲍姆-韦尔奇算法*（Baum-Welch Algorithm）*：

        * 不同的隐含马尔可夫模型可以产生相同的输出信号。

        * 首先找到一组能够产生输出序列$O$的模型参数作为初始模型$M_{\theta 0}$。

        * 假定解决了第一个和第二个问题，可以计算出这个模型产生$O$的概率$P(O|M_{\theta 0})$，还可找到这个模型产生$O$的所有可能路径以及这些路径的概率。可将其看作是“标注的训练数据”，利用
          $$
          P(o_t|s_t)\approx \frac {\#(o_t,s_t)}{\#(s_t)} \ \ 足够多人工标记数据统计估算（有监督训练）
          \\ P(s_t|s_{t-1})\approx \frac{\#(s_{t-1},s_t)}{\#(s_{t-1})} \ \ 同统计语言模型的训练方法P(w_i|w_{i-1})\approx \frac{\#(w_{i-1},w_i)}{\#(w_{i-1})}
          $$
          计算出一组新的模型参数$M_{\theta 1}$,使得
          $$
          P(O|M_{\theta 1})>P(O|M_{\theta 0})
          $$

        * 之后不断迭代，直到模型质量不再有明显提升。

        鲍姆-韦尔奇算法每次迭代是不断估计使得输出的概率最大化*（Expectation Maximization）*，简称EM过程。EM过程只能保证算法一定能收敛到局部最优解。有监督的训练可以收敛到全局最优点。

     ==使用隐含马尔可夫模型包括一个训练算法（鲍姆-韦尔奇算法）和使用时的解码算法（维特比算法）。==

   ​

## 第6章：信息的度量和作用

1. 信息熵

   信息量就等于不确定性的多少。“比特”（Bit）来度量信息量，信息量的比特数和所有可能情况的对数函数$log_2$有关。熵的定义如下：
   $$
   H(X)=-\sum_{x\in X}{P(x)log_2 P(x)}
   $$
   变量的不确定性越大，熵也越大，所需的信息量就越多。信息熵具有三条性质：

   * 单调性，发生概率越高的事件，其所携带的信息熵越低。

   * 非负性，信息熵不能为负。

   * 累加性，多随机事件同时发生存在的总不确定性的度量可以表示为各事件不确定性的度量和：
     $$
     if,\ \ P(A,B)=P(A)\cdot P(B) \\then,\ H(A,B)=H(A)+H(B)
     $$



   引申：霍夫曼编码（为出现概率高的字符分配短码）

2. 信息的作用

   信息是消除系统不确定性的唯一办法。自然语言处理的大量问题就是寻找相关的信息消除不确定性。

   假定$X$和$Y$是两个随机变量，定义在$Y$的条件下的条件熵（Conditional Entropy）为：
   $$
   H(X|Y)=-\sum_{x\in X,y\in Y}{P(x,y)log_2 P(x|y)}\\H(X)\geq H(X|Y)
   $$
   ​

3. 互信息

   假定又两个随机事件$X$和$Y$，它们的互信息定义如下：
   $$
   I(X;Y)=\sum_{x\in X,y\in Y} P(x,y)log_2\frac{P(x,y)}{P(x)P(y)}\\ I(X;Y)=H(X)-H(X|Y)
   $$
   两个事件相关性的量化度量就是在了解其中一个$Y$的前提下，对消除另一个事件$X$不确定性所提供的信息量。互信息是取值在0到$min(H(X),H(Y))$之间的函数。当$X$和$Y$完全相关时，$I(X,Y)=H(X), \text 且H(X)=H(Y)$；当两者无关时，$I(X,Y)=0$。

   使用互信息可以有效地解决词义的二义性。

4. 相对熵（Relative Entropy or Kullback-Leibler Divergence）

   和互信息不同，相对熵用来衡量两个取值为正数的函数的相似性：
   $$
   KL(f(x)||g(x))=\sum_{x\in X}f(x)\cdot log\frac{f(x)}{g(x)}
   $$

   * 对于两个完全相同的函数，它们的相对熵等于零。

   * 相对熵越大，两个函数差异越大；相对熵越小，两个函数差异越小。

   * 对于概率分布或者概率密度函数，如果取值均大于零，相对熵可以度量两个随机分布的差异性。

   * 相对熵是不对称的：
     $$
     KL(f(x)||g(x))\neq KL(g(x)||f(x))
     $$
     詹森和香农提出新的相对熵计算方法：
     $$
     JS(f(x)||g(x))=\frac 1 2 [KL(f(x)||g(x))+KL(g(x)||f(x))]
     $$



   相对熵的应用：

   * 衡量两段信息的相似程度，相对熵越小，说明两段信息越接近。
   * 衡量两个常用词在不同文本中的概率分布，看它们是否同义。
   * 词频率-逆向文档频率(TF-IDF)。

5. 小结

   信息熵能直接用于衡量统计语言模型的好坏。

   对高阶的语言模型，应该用条件熵。

   如从训练语料和真实应用的文本中得到的概率函数有偏差，需要再引入相对熵。

   语言模型负载度（Perplexity）：在给定上下文的条件下，句子中每个位置平均可以选择的单词数量。一个模型的复杂度越小，每个位置的词就越确定，模型越好。



##第7章：贾里尼克和现代语言处理

* “巨人”的力量
* 将统计方法应用于自然语言处理的必然
* IBM最早提出统计语音识别和自然语言处理的历史必然性：计算能力、语料和人。


## 第8章：简单之美，布尔代数和搜索引擎

建立一个搜索引擎需要做的是：自动下载尽可能多的网页；建立快速有效的索引；根据相关性对网页进行公平准则的排序。（下载，索引，排序）

索引在搜索引擎中最基础也最为重要。

1. 布尔代数

   运算元素：1和0 。基本运算：与(AND)，或(OR)，非(NOT) 。

   布尔运算与文献检索的关系，对于一个用户输入的关键词，搜索引擎要判断每篇文献是否含有这个关键词，从而对每篇文献分配一个逻辑值。

2. 索引

   图书馆索引卡片类比互联网搜索引擎。

   最简单的索引结构：有多少文献就用多少位数对应。1代表相应文献有这个关键字，0代表没有。

   互联网搜素引擎，每个网页就是一个文献。索引量巨大，在万亿字节的量级。索引需要通过分布式的方式存储到不同服务器上。普遍做法是根据网页序号将索引分成很多份（Shards），分别存储在不同的服务器中，服务器并行处理用户请求。




## 第9章：图论和网络爬虫

离散数学四大分支：数理逻辑（基于布尔运算），集合论，图论，近世代数。

如何自动下载互联网网页？（图论的遍历Traverse算法）

1. 图论

   一种图的遍历算法“广度优先搜索”（Breadth-First Search，BFS），尽可能广地访问与每个节点直接链接的其他节点。 It starts at the tree root and explores the neighbor nodes first, before moving to the next level neighbours.

   另一种图的遍历算法“深度优先搜索”（Depth-First Search，DFS）,"一路走到黑"。One starts at the root and explores as far as possible along each branch before backtracking.

2. 网络爬虫（Web Crawlers）

   互联网看作图：每个网页是一个节点，每个Hyperlinks当作连接网页的弧。

   网络爬虫：从任何一个网页出发，用图的遍历算法，自动访问每个网页并把它们存起来。在网络爬虫中，使用“散列表”(Hash Table)来记录网页是否下载过，避免重复下载。

3. 图论的两点补充

   3.1 欧拉七巧问题

   图中每一个顶点，与之相连的边的数量称为它的度（Degree）。

   定理：如果一个图能够从一个顶点出发，每条边不重复地遍历一遍回到这个顶点，那每一顶点的度必须是偶数。

   3.2 构建网络爬虫的工程要点

   * BFS还是DFS

     在不考虑时间因素，互联网静态不变的情况下，这两个算法的时间复杂度大概相同「$O(V+E)$是节点数量$V$和边的数量$E$之和的线性函数」

     现实网络爬虫问题应定义为：如何在有限的时间内最多地爬下最重要的网页。这时显然BFS优于DFS。

     涉及到爬虫的分布式结构和网络通信的<u>握手</u>（下载服务器和网站的服务器建立通信的过程）成本：握手次数过多，下载效率下降。网络爬虫是由成千上万服务器组成的分布系统。对于一个网站需要特定的服务器<u>先下载完，再下载别的网站</u>(DFS)。

     总结，网络爬虫对网页遍历不是简单的BFS或DFS，而是有相对复杂的下载优先级排序方法。由调度系统（Scheduler）管理。

   * 页面分析和URL的提取

     一个网页下载完，需要提取网页中的URL，再加入下载队列中。

     早期网页直接用HTML编写，URL方便提取。现在很多使用脚本语言（如JS）生成，解析困难，需要<u>模拟浏览器运行一个网页</u>（解析），然后得到隐含的URL。

   * 记录已下载过的网页——URL表

     为了防止一个网页（节点）被下载多次，用散列表记录已下载过的网页。

     上千台服务器一起下载网页，维护散列表很困难。这张表可能会大到一台服务器存不下；每个下载服务器在下载前和完成后都要访问和维护散列表，以免不同服务器重复工作，对存储散列表的服务器的通信成了整个爬虫系统的瓶颈。

     解决方法：

     * 明确每台下载服务器的分工，即调度时一个URL的下载任务对应一个服务器，以免重复判断。
     * 在明确分工基础上，批处理判断URL是否下载。（如每次向散列表发送一大批询问，或者每次更新一大批散列表内容）



## 第10章：PageRank（Google的民主表决式网页排名技术）

搜索结果的排名取决于两组信息：关于网页质量信息（Quality）和这个查询与每个网页的相关性信息（Relevance）。

1. PageRank算法原理（衡量网页质量）

   PageRank的核心思想：如果一个网页被很多其他网页所链接，说明它受到普遍的承认和信赖，它的搜索排名就高。且网页排名高的网站贡献的链接权重大。

   * 网页本身的排名决定了其指向其他网页的权重，指向同一网页的权重相加为该网页的排名。
   * 二维矩阵相乘，假定所有网页排名相同，根据初始值不断迭代排名。
   * 二维矩阵理论上有网页数量平方多个元素，稀疏矩阵计算技巧简化计算量。MapReduce工具自动化PageRank并行计算。
   * 今天决定搜索质量最有用的信息是用户的点击数据。

2. PageRank的计算方法

   假定向量$B=\begin{bmatrix}b_1\\b_2\\ \vdots \\ b_N\end{bmatrix}$为第一，第二，$\cdots$，第N个网页的排名。矩阵$A=\begin{bmatrix}a_{11}, \cdots, a_{1n}, \cdots, a_{1M}\\ \cdots \\ a_{m1}, \cdots, a_{mn}, \cdots, a_{mM}\\ \cdots \\ a_{M1}, \cdots, a_{Mn}, \cdots, a_{MM}\end{bmatrix}$为网页之间链接的数目，其中$a_{mn}$代表第$m$个网页指向第$n$个网页的链接数量。$A$是已知，$B$是未知的。

   初始假设，所有网页的排名都是1/N，即$B_0=(\frac 1 N,\frac 1 N,\cdots,\frac 1 N)$。

   $B_i$是第i次迭代的结果：$B_i=A\cdot B_{i-1}$。$B_i$最终会收敛，即$B_i$无限接近于$B$，$B=B\times A$。此时停止迭代，算法结束。一般迭代10次基本就收敛了。

   由于网页之间链接数量相比互联网的规模非常稀疏，因此计算网页的排名也需要对零概率事件进行平滑处理。即$B_i=[\frac \alpha N \cdot I+(1-\alpha)A]\cdot B_{i-1}$，其中N是互联网网页数量，$\alpha$是一个较小的常数，$I$是单位矩阵。

   ​



## 第11章：如何确定网页和查询的相关性

